<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "5e9775ee91bde7d44577891d5f11c4c5",
  "translation_date": "2025-09-03T20:43:48+00:00",
  "source_file": "8.3 Responsible AI.md",
  "language_code": "ar"
}
-->
# الذكاء الاصطناعي المسؤول

[![شاهد الفيديو](../../translated_images/8-3_placeholder.9a5623e020ef9751bfd82c06e3014edc976e2b2dc6ac5836571e63873a3c28b4.ar.png)](https://learn-video.azurefd.net/vod/player?id=b7517901-8f81-4475-b586-385a361c51e8)

## ما هو الذكاء الاصطناعي المسؤول وكيف يرتبط بأمان الذكاء الاصطناعي؟

يشير الذكاء الاصطناعي المسؤول إلى تطوير واستخدام الذكاء الاصطناعي بطريقة أخلاقية وشفافة ومتوافقة مع القيم المجتمعية. يتضمن مبادئ مثل العدالة والمساءلة والمتانة، مما يضمن تصميم وتشغيل أنظمة الذكاء الاصطناعي لتحقيق فوائد للأفراد والمجتمعات والمجتمع ككل.

العلاقة بين الذكاء الاصطناعي المسؤول وأمان الذكاء الاصطناعي مهمة لأن:

- **الاعتبارات الأخلاقية**: يتضمن الذكاء الاصطناعي المسؤول اعتبارات أخلاقية تؤثر مباشرة على الأمان، مثل الخصوصية وحماية البيانات. ضمان احترام أنظمة الذكاء الاصطناعي لخصوصية المستخدم وتأمين البيانات الشخصية هو جانب أساسي من الذكاء الاصطناعي المسؤول.
- **المتانة والموثوقية**: يجب أن تكون أنظمة الذكاء الاصطناعي قوية ضد التلاعب والهجمات، وهو مبدأ أساسي لكل من الذكاء الاصطناعي المسؤول وأمان الذكاء الاصطناعي. يشمل ذلك الحماية من الهجمات العدائية وضمان سلامة عمليات اتخاذ القرار في الذكاء الاصطناعي.
- **الشفافية وقابلية التفسير**: جزء من الذكاء الاصطناعي المسؤول هو ضمان أن تكون أنظمة الذكاء الاصطناعي شفافة ويمكن تفسير قراراتها. هذا أمر بالغ الأهمية للأمان، حيث يحتاج أصحاب المصلحة إلى فهم كيفية عمل أنظمة الذكاء الاصطناعي للثقة في تدابير الأمان الخاصة بها.
- **المساءلة**: يجب أن تكون أنظمة الذكاء الاصطناعي مسؤولة عن أفعالها، مما يعني أنه يجب أن تكون هناك آليات لتتبع القرارات وتصحيح أي مشكلات. يتماشى هذا مع ممارسات الأمان التي تراقب وتدقق أنشطة النظام لمنع ومعالجة الانتهاكات.

بالمجمل، الذكاء الاصطناعي المسؤول وأمان الذكاء الاصطناعي مترابطان، حيث تعزز ممارسات الذكاء الاصطناعي المسؤول أمان أنظمة الذكاء الاصطناعي والعكس صحيح. تطبيق مبادئ الذكاء الاصطناعي المسؤول يساعد في إنشاء أنظمة ذكاء اصطناعي ليست فقط أخلاقية بل أيضًا أكثر أمانًا ضد التهديدات المحتملة.

## كيف يمكنني ضمان أن يكون نظام الذكاء الاصطناعي الخاص بي آمنًا وأخلاقيًا؟

ضمان أن يكون نظام الذكاء الاصطناعي الخاص بك آمنًا وأخلاقيًا يتطلب نهجًا متعدد الجوانب يشمل الخطوات التالية:

- **الالتزام بالمبادئ الأخلاقية**: اتبع الإرشادات الأخلاقية المعتمدة التي تؤكد على رفاهية الإنسان والمجتمع والبيئة؛ العدالة؛ حماية الخصوصية؛ الموثوقية؛ الشفافية؛ قابلية الطعن؛ والمساءلة.

- **تنفيذ تدابير أمان قوية**: استخدم اختبارات الأمان الاستباقية وبرامج إدارة الثقة والمخاطر والأمان للذكاء الاصطناعي لحماية النظام من التهديدات والثغرات.

- **إشراك أصحاب المصلحة المتنوعين**: قم بإشراك مجموعة واسعة من المشاركين في عملية تطوير الذكاء الاصطناعي، بما في ذلك الأخلاقيين، علماء الاجتماع، وممثلين عن المجتمعات المتأثرة لضمان مراعاة وجهات النظر والقيم المتنوعة.

- **ضمان الشفافية وقابلية التفسير**: تأكد من أن عمليات اتخاذ القرار في الذكاء الاصطناعي شفافة ويمكن تفسيرها، مما يعزز الثقة ويسهل تحديد التحيزات أو الأخطاء المحتملة.

- **الحفاظ على خصوصية البيانات**: احمِ خصوصية البيانات وأصالتها من خلال التشفير وتدابير حماية البيانات الأخرى لاحترام حقوق الخصوصية للمستخدمين.

- **تمكين الإشراف البشري**: قم بتطبيق آليات للإشراف البشري للسماح بالطعن في القرارات التي تتخذها أنظمة الذكاء الاصطناعي وضمان المساءلة.

- **البقاء على اطلاع بأمان الذكاء الاصطناعي**: تابع أحدث الأبحاث والمناقشات حول أمان الذكاء الاصطناعي لفهم المشهد المتطور لأمان وأخلاقيات الذكاء الاصطناعي.

- **الامتثال للوائح**: تأكد من أن نظام الذكاء الاصطناعي الخاص بك يتوافق مع جميع القوانين واللوائح ذات الصلة، والتي قد تشمل قوانين حماية البيانات، قوانين مكافحة التمييز، وإرشادات خاصة بالصناعة.

## هل يمكن أن تعطيني أمثلة على مشكلة أمان ناجمة عن الاستخدام غير الأخلاقي للذكاء الاصطناعي؟

فيما يلي بعض الأمثلة على مشاكل الأمان التي يمكن أن تنشأ عن الاستخدام غير الأخلاقي للذكاء الاصطناعي:

- **اتخاذ قرارات متحيزة**: يمكن لأنظمة الذكاء الاصطناعي أن تعزز وتضخم التحيزات الموجودة إذا تم تدريبها على مجموعات بيانات متحيزة. على سبيل المثال، إذا تم تدريب محرك بحث على بيانات تعكس الصور النمطية المجتمعية، فقد يعرض نتائج بحث متحيزة، مما يؤدي إلى معاملة غير عادلة أو تمييز.

- **الذكاء الاصطناعي في الأنظمة القضائية**: استخدام الذكاء الاصطناعي في اتخاذ القرارات القانونية يمكن أن يثير مخاوف أخلاقية، خاصة إذا كانت عملية اتخاذ القرار في الذكاء الاصطناعي تفتقر إلى الشفافية أو تتأثر ببيانات متحيزة. قد يؤدي ذلك إلى نتائج قانونية غير عادلة وانتهاك حقوق الأفراد.

- **التلاعب بأنظمة الذكاء الاصطناعي**: يمكن أن تكون أنظمة الذكاء الاصطناعي عرضة للهجمات العدائية، حيث تؤدي تعديلات طفيفة على بيانات الإدخال إلى نتائج غير صحيحة. على سبيل المثال، يمكن تضليل المركبات ذاتية القيادة لتفسير إشارات المرور بشكل خاطئ، مما يؤدي إلى مخاطر على السلامة.

- **المراقبة باستخدام الذكاء الاصطناعي**: يمكن أن يؤدي نشر الذكاء الاصطناعي لأغراض المراقبة إلى انتهاكات الخصوصية، خاصة إذا تم استخدامه دون موافقة مناسبة أو بطرق تنتهك الحريات الفردية. قد يكون هذا مشكلة بشكل خاص في الأنظمة الاستبدادية التي قد تستخدم الذكاء الاصطناعي لمراقبة وقمع المعارضة.

توضح هذه الأمثلة أهمية الاعتبارات الأخلاقية في تطوير ونشر أنظمة الذكاء الاصطناعي لمنع مشاكل الأمان وحماية حقوق وخصوصية الأفراد.

## قراءة إضافية

- [Microsoft Responsible AI Standard v2 General Requirements](https://query.prod.cms.rt.microsoft.com/cms/api/am/binary/RE5cmFl?culture=en-us&country=us&WT.mc_id=academic-96948-sayoung)
- [Responsible AI (mit.edu)](https://sloanreview.mit.edu/big-ideas/responsible-ai/)
- [13 Principles for Using AI Responsibly (hbr.org)](https://hbr.org/2023/06/13-principles-for-using-ai-responsibly)

---

**إخلاء المسؤولية**:  
تم ترجمة هذا المستند باستخدام خدمة الترجمة بالذكاء الاصطناعي [Co-op Translator](https://github.com/Azure/co-op-translator). بينما نسعى لتحقيق الدقة، يرجى العلم أن الترجمات الآلية قد تحتوي على أخطاء أو معلومات غير دقيقة. يجب اعتبار المستند الأصلي بلغته الأصلية المصدر الرسمي. للحصول على معلومات حاسمة، يُوصى بالاستعانة بترجمة بشرية احترافية. نحن غير مسؤولين عن أي سوء فهم أو تفسيرات خاطئة ناتجة عن استخدام هذه الترجمة.