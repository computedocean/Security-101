<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "b6bb7175672298d1e2f73ba7e0006f95",
  "translation_date": "2025-09-03T21:37:41+00:00",
  "source_file": "8.2 AI security capabilities.md",
  "language_code": "uk"
}
-->
# Можливості безпеки штучного інтелекту

[![Дивитися відео](../../translated_images/8-2_placeholder.bc988ce5dff1726a8b6f8c00b1250865ca23d02aa5cb11fb879ed1194702c99a.uk.png)](https://learn-video.azurefd.net/vod/player?id=e0a6f844-d884-4f76-99bd-4ce9f7f73d22)

## Які інструменти та можливості ми маємо для забезпечення безпеки систем штучного інтелекту на даний момент?

На даний момент існує кілька інструментів і можливостей для забезпечення безпеки систем штучного інтелекту:

-   **Counterfit**: Відкритий інструмент автоматизації для тестування безпеки систем штучного інтелекту, створений для допомоги організаціям у проведенні оцінки ризиків безпеки штучного інтелекту та забезпеченні стійкості їхніх алгоритмів.
-   **Інструменти для протидії атакам на машинне навчання**: Ці інструменти оцінюють стійкість моделей машинного навчання до атак, допомагаючи виявляти та усувати вразливості.
-   **Набори інструментів для безпеки штучного інтелекту**: Існують відкриті набори інструментів, які надають ресурси для забезпечення безпеки систем штучного інтелекту, включаючи бібліотеки та фреймворки для впровадження заходів безпеки.
-   **Платформи співпраці**: Партнерства між компаніями та спільнотами штучного інтелекту для розробки сканерів безпеки, специфічних для штучного інтелекту, та інших інструментів для захисту ланцюга постачання штучного інтелекту.

Ці інструменти та можливості є частиною зростаючої галузі, спрямованої на підвищення безпеки систем штучного інтелекту перед різноманітними загрозами. Вони представляють собою поєднання досліджень, практичних інструментів і співпраці в індустрії, спрямованих на вирішення унікальних викликів, які ставлять технології штучного інтелекту.

## Що таке тестування червоної команди для штучного інтелекту? Чим воно відрізняється від традиційного тестування червоної команди?

Тестування червоної команди для штучного інтелекту відрізняється від традиційного тестування червоної команди кількома ключовими аспектами:

-   **Фокус на системах штучного інтелекту**: Тестування червоної команди для штучного інтелекту спеціально спрямоване на унікальні вразливості систем штучного інтелекту, таких як моделі машинного навчання та канали даних, а не на традиційну ІТ-інфраструктуру.
-   **Тестування поведінки штучного інтелекту**: Воно включає тестування того, як системи штучного інтелекту реагують на незвичайні або несподівані вхідні дані, що може виявити вразливості, які можуть бути використані зловмисниками.
-   **Дослідження збоїв штучного інтелекту**: Тестування червоної команди для штучного інтелекту розглядає як зловмисні, так і добросовісні збої, враховуючи ширший набір сценаріїв і потенційних збоїв системи, а не лише порушення безпеки.
-   **Ін'єкція запитів і генерація контенту**: Воно також включає перевірку на збої, такі як ін'єкція запитів, коли зловмисники маніпулюють системами штучного інтелекту для створення шкідливого або необґрунтованого контенту.
-   **Етичний і відповідальний штучний інтелект**: Це частина забезпечення відповідального штучного інтелекту за дизайном, що гарантує стійкість систем штучного інтелекту до спроб змусити їх діяти неналежним чином.

Загалом, тестування червоної команди для штучного інтелекту є розширеною практикою, яка охоплює не лише перевірку на вразливості безпеки, але й тестування інших типів збоїв систем, специфічних для технологій штучного інтелекту. Це важливий етап у розробці безпечніших систем штучного інтелекту шляхом розуміння та усунення нових ризиків, пов'язаних із впровадженням штучного інтелекту.

## Додаткові матеріали

 - [Microsoft AI Red Team building future of safer AI | Microsoft Security Blog](https://www.microsoft.com/en-us/security/blog/2023/08/07/microsoft-ai-red-team-building-future-of-safer-ai/?WT.mc_id=academic-96948-sayoung)
 - [Announcing Microsoft’s open automation framework to red team generative AI Systems | Microsoft Security Blog](https://www.microsoft.com/en-us/security/blog/2024/02/22/announcing-microsofts-open-automation-framework-to-red-team-generative-ai-systems/?WT.mc_id=academic-96948-sayoung)
 - [AI Security Tools: The Open-Source Toolkit | Wiz](https://www.wiz.io/academy/ai-security-tools)

---

**Відмова від відповідальності**:  
Цей документ був перекладений за допомогою сервісу автоматичного перекладу [Co-op Translator](https://github.com/Azure/co-op-translator). Хоча ми прагнемо до точності, будь ласка, майте на увазі, що автоматичні переклади можуть містити помилки або неточності. Оригінальний документ на його рідній мові слід вважати авторитетним джерелом. Для критичної інформації рекомендується професійний людський переклад. Ми не несемо відповідальності за будь-які непорозуміння або неправильні тлумачення, що виникають внаслідок використання цього перекладу.