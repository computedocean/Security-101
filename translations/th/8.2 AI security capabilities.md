<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "b6bb7175672298d1e2f73ba7e0006f95",
  "translation_date": "2025-09-04T01:05:14+00:00",
  "source_file": "8.2 AI security capabilities.md",
  "language_code": "th"
}
-->
# ความสามารถด้านความปลอดภัยของ AI

[![ดูวิดีโอ](../../translated_images/8-2_placeholder.bc988ce5dff1726a8b6f8c00b1250865ca23d02aa5cb11fb879ed1194702c99a.th.png)](https://learn-video.azurefd.net/vod/player?id=e0a6f844-d884-4f76-99bd-4ce9f7f73d22)

## เครื่องมือและความสามารถที่เรามีในปัจจุบันเพื่อรักษาความปลอดภัยให้กับระบบ AI มีอะไรบ้าง?

ในปัจจุบัน มีเครื่องมือและความสามารถหลายอย่างที่สามารถใช้เพื่อรักษาความปลอดภัยให้กับระบบ AI:

-   **Counterfit**: เครื่องมือโอเพ่นซอร์สสำหรับการทดสอบความปลอดภัยของระบบ AI ที่ช่วยให้องค์กรสามารถประเมินความเสี่ยงด้านความปลอดภัยของ AI และตรวจสอบความแข็งแกร่งของอัลกอริทึมได้
-   **เครื่องมือ Adversarial Machine Learning**: เครื่องมือเหล่านี้ช่วยประเมินความแข็งแกร่งของโมเดลการเรียนรู้ของเครื่องต่อการโจมตีแบบ adversarial และช่วยระบุและลดช่องโหว่
-   **AI Security Toolkits**: มีชุดเครื่องมือโอเพ่นซอร์สที่ให้ทรัพยากรสำหรับการรักษาความปลอดภัยของระบบ AI รวมถึงไลบรารีและเฟรมเวิร์กสำหรับการนำมาตรการรักษาความปลอดภัยไปใช้
-   **แพลตฟอร์มความร่วมมือ**: การร่วมมือระหว่างบริษัทและชุมชน AI เพื่อพัฒนาเครื่องมือเฉพาะทาง เช่น AI security scanners และเครื่องมืออื่น ๆ เพื่อรักษาความปลอดภัยในห่วงโซ่อุปทานของ AI

เครื่องมือและความสามารถเหล่านี้เป็นส่วนหนึ่งของสาขาที่กำลังเติบโต ซึ่งมุ่งเน้นไปที่การเพิ่มความปลอดภัยให้กับระบบ AI ต่อภัยคุกคามหลากหลายรูปแบบ โดยเป็นการผสมผสานระหว่างงานวิจัย เครื่องมือที่ใช้งานได้จริง และความร่วมมือในอุตสาหกรรม เพื่อจัดการกับความท้าทายเฉพาะที่เกิดจากเทคโนโลยี AI

## แล้ว AI red teaming ล่ะ? มันแตกต่างจากการทำ red teaming ด้านความปลอดภัยแบบดั้งเดิมอย่างไร?

AI red teaming มีความแตกต่างจากการทำ red teaming ด้านความปลอดภัยแบบดั้งเดิมในหลายแง่มุมสำคัญ:

-   **มุ่งเน้นที่ระบบ AI**: AI red teaming มุ่งเป้าไปที่ช่องโหว่เฉพาะของระบบ AI เช่น โมเดลการเรียนรู้ของเครื่องและกระบวนการจัดการข้อมูล แทนที่จะเป็นโครงสร้างพื้นฐาน IT แบบดั้งเดิม
-   **การทดสอบพฤติกรรมของ AI**: เป็นการทดสอบว่าระบบ AI ตอบสนองต่ออินพุตที่ผิดปกติหรือไม่คาดคิดอย่างไร ซึ่งสามารถเผยให้เห็นช่องโหว่ที่ผู้โจมตีอาจใช้ประโยชน์ได้
-   **การสำรวจความล้มเหลวของ AI**: AI red teaming พิจารณาทั้งความล้มเหลวที่เกิดจากการโจมตีและความล้มเหลวที่เกิดขึ้นโดยไม่ได้ตั้งใจ โดยมองในมุมมองที่กว้างกว่าการละเมิดความปลอดภัยเพียงอย่างเดียว
-   **การโจมตีด้วย prompt injection และการสร้างเนื้อหา**: รวมถึงการตรวจสอบความล้มเหลว เช่น prompt injection ซึ่งผู้โจมตีสามารถปรับเปลี่ยนระบบ AI ให้สร้างเนื้อหาที่เป็นอันตรายหรือไม่ถูกต้อง
-   **AI ที่มีจริยธรรมและความรับผิดชอบ**: เป็นส่วนหนึ่งของการออกแบบ AI อย่างมีความรับผิดชอบ เพื่อให้มั่นใจว่าระบบ AI มีความแข็งแกร่งต่อความพยายามที่จะทำให้มันทำงานในลักษณะที่ไม่พึงประสงค์

โดยรวมแล้ว AI red teaming เป็นกระบวนการที่ขยายขอบเขตออกไป ไม่เพียงแต่ครอบคลุมการตรวจสอบช่องโหว่ด้านความปลอดภัย แต่ยังรวมถึงการทดสอบความล้มเหลวประเภทอื่น ๆ ที่เฉพาะเจาะจงกับเทคโนโลยี AI ด้วย ถือเป็นส่วนสำคัญในการพัฒนาระบบ AI ที่ปลอดภัยยิ่งขึ้น โดยการทำความเข้าใจและลดความเสี่ยงใหม่ ๆ ที่เกี่ยวข้องกับการใช้งาน AI

## อ่านเพิ่มเติม

 - [Microsoft AI Red Team building future of safer AI | Microsoft Security Blog](https://www.microsoft.com/en-us/security/blog/2023/08/07/microsoft-ai-red-team-building-future-of-safer-ai/?WT.mc_id=academic-96948-sayoung)
 - [Announcing Microsoft’s open automation framework to red team generative AI Systems | Microsoft Security Blog](https://www.microsoft.com/en-us/security/blog/2024/02/22/announcing-microsofts-open-automation-framework-to-red-team-generative-ai-systems/?WT.mc_id=academic-96948-sayoung)
 - [AI Security Tools: The Open-Source Toolkit | Wiz](https://www.wiz.io/academy/ai-security-tools)

---

**ข้อจำกัดความรับผิดชอบ**:  
เอกสารนี้ได้รับการแปลโดยใช้บริการแปลภาษา AI [Co-op Translator](https://github.com/Azure/co-op-translator) แม้ว่าเราจะพยายามให้การแปลมีความถูกต้อง แต่โปรดทราบว่าการแปลอัตโนมัติอาจมีข้อผิดพลาดหรือความไม่แม่นยำ เอกสารต้นฉบับในภาษาดั้งเดิมควรถือเป็นแหล่งข้อมูลที่เชื่อถือได้ สำหรับข้อมูลที่สำคัญ แนะนำให้ใช้บริการแปลภาษาจากผู้เชี่ยวชาญ เราไม่รับผิดชอบต่อความเข้าใจผิดหรือการตีความที่ผิดพลาดซึ่งเกิดจากการใช้การแปลนี้