<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "b6bb7175672298d1e2f73ba7e0006f95",
  "translation_date": "2025-09-03T18:45:50+00:00",
  "source_file": "8.2 AI security capabilities.md",
  "language_code": "fr"
}
-->
# Capacités de sécurité de l'IA

[![Regarder la vidéo](../../translated_images/8-2_placeholder.bc988ce5dff1726a8b6f8c00b1250865ca23d02aa5cb11fb879ed1194702c99a.fr.png)](https://learn-video.azurefd.net/vod/player?id=e0a6f844-d884-4f76-99bd-4ce9f7f73d22)

## Quels outils et capacités avons-nous actuellement pour sécuriser les systèmes d'IA ?

Actuellement, plusieurs outils et capacités sont disponibles pour sécuriser les systèmes d'IA :

-   **Counterfit** : Un outil d'automatisation open-source pour les tests de sécurité des systèmes d'IA, conçu pour aider les organisations à réaliser des évaluations des risques de sécurité de l'IA et à garantir la robustesse de leurs algorithmes.
-   **Outils d'apprentissage automatique adversarial** : Ces outils évaluent la robustesse des modèles d'apprentissage automatique face aux attaques adversariales, aidant à identifier et à atténuer les vulnérabilités.
-   **Kits d'outils de sécurité pour l'IA** : Des kits open-source sont disponibles, offrant des ressources pour sécuriser les systèmes d'IA, notamment des bibliothèques et des frameworks pour mettre en œuvre des mesures de sécurité.
-   **Plateformes collaboratives** : Des partenariats entre entreprises et communautés d'IA pour développer des scanners de sécurité spécifiques à l'IA et d'autres outils visant à sécuriser la chaîne d'approvisionnement de l'IA.

Ces outils et capacités font partie d'un domaine en pleine expansion dédié à renforcer la sécurité des systèmes d'IA contre une variété de menaces. Ils représentent une combinaison de recherche, d'outils pratiques et de collaborations industrielles visant à relever les défis uniques posés par les technologies d'IA.

## Qu'en est-il du red teaming pour l'IA ? En quoi cela diffère-t-il du red teaming traditionnel en sécurité ?

Le red teaming pour l'IA diffère du red teaming traditionnel en sécurité sur plusieurs points clés :

-   **Ciblage des systèmes d'IA** : Le red teaming pour l'IA se concentre spécifiquement sur les vulnérabilités uniques des systèmes d'IA, comme les modèles d'apprentissage automatique et les pipelines de données, plutôt que sur l'infrastructure informatique traditionnelle.
-   **Test du comportement de l'IA** : Il consiste à tester la manière dont les systèmes d'IA réagissent à des entrées inhabituelles ou inattendues, ce qui peut révéler des vulnérabilités exploitables par des attaquants.
-   **Exploration des défaillances de l'IA** : Le red teaming pour l'IA examine à la fois les défaillances malveillantes et bénignes, en considérant un éventail plus large de scénarios et de défaillances potentielles au-delà des simples violations de sécurité.
-   **Injection de prompts et génération de contenu** : Il inclut également des tests pour des défaillances telles que l'injection de prompts, où des attaquants manipulent les systèmes d'IA pour produire du contenu nuisible ou non fondé.
-   **Éthique et IA responsable** : Cela fait partie de l'assurance d'une IA responsable dès la conception, en veillant à ce que les systèmes d'IA soient robustes face aux tentatives de les faire agir de manière non intentionnelle.

En somme, le red teaming pour l'IA est une pratique élargie qui ne se limite pas à la recherche de vulnérabilités de sécurité, mais inclut également des tests pour d'autres types de défaillances spécifiques aux technologies d'IA. C'est une étape essentielle pour développer des systèmes d'IA plus sûrs en comprenant et en atténuant les risques nouveaux liés au déploiement de l'IA.

## Lectures complémentaires

 - [Microsoft AI Red Team building future of safer AI | Microsoft Security Blog](https://www.microsoft.com/en-us/security/blog/2023/08/07/microsoft-ai-red-team-building-future-of-safer-ai/?WT.mc_id=academic-96948-sayoung)
 - [Announcing Microsoft’s open automation framework to red team generative AI Systems | Microsoft Security Blog](https://www.microsoft.com/en-us/security/blog/2024/02/22/announcing-microsofts-open-automation-framework-to-red-team-generative-ai-systems/?WT.mc_id=academic-96948-sayoung)
 - [AI Security Tools: The Open-Source Toolkit | Wiz](https://www.wiz.io/academy/ai-security-tools)

---

**Avertissement** :  
Ce document a été traduit à l'aide du service de traduction automatique [Co-op Translator](https://github.com/Azure/co-op-translator). Bien que nous nous efforcions d'assurer l'exactitude, veuillez noter que les traductions automatisées peuvent contenir des erreurs ou des inexactitudes. Le document original dans sa langue d'origine doit être considéré comme la source faisant autorité. Pour des informations critiques, il est recommandé de faire appel à une traduction humaine professionnelle. Nous déclinons toute responsabilité en cas de malentendus ou d'interprétations erronées résultant de l'utilisation de cette traduction.